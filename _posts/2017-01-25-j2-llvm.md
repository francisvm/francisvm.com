---
layout:     post
title:      Write yourself a toolchain using LLVM
date:       2017-01-20 23:42:00
summary:    Add support for the j2 Core (SuperH 2) architecture to LLVM.
categories: llvm
---

The aim of [this project](https://github.com/thegameg/j2-llvm) is learn how the
[LLVM compiler infrastructure](http://llvm.org/) works, by adding support for
a new architecture, the [j2 Core (SuperH 2)](http://j-core.org/).

# LLVM Compiler Infrastructure

The compiler infrastructure is based on many sub-projects, that often aim at
sharing as much code as possible, while shipping both binaries and libraries.

What's so great about LLVM, is that its components can be used separately
by working out-of-tree, and linking with the desired libraries.

## LLVM Core

#### Analysis / Transforms

The LLVM core libraries start by working with the famous intermediate
representation, the `LLVM IR`, and provides many generic optimization passes.

Basically, any front-end that can generate `LLVM IR` can take advantage of
(almost) all the optimizations that a mature _compiler_ (like `clang`) performs.

Take a look at the [LLVM chapter of The Architecture of Open Source
Applications](http://www.aosabook.org/en/llvm.html):

![LLVM IR]({{ site.baseurl }}/assets/j2-llvm/llvm-ir.png)

Retargetability is (imo) by far one of the most interesting features of LLVM,
along with its modularity.

#### Codegen

After the optimizations, another huge part of the core libraries is [the code
generator](http://llvm.org/docs/CodeGenerator.html). The code generator is also
designed to use a generic algorithm, that queries target-specific code when
needed.

#### MC

You might think this is where the core libraries stop, since a classic compiler
stops at code generation.

What's so great about LLVM, is that many of its sub-projects, and the
code-generator itself, deal with **Machine Code**, so a machine code library
called [MC](http://blog.llvm.org/2010/04/intro-to-llvm-mc-project.html) has
landed in LLVM to handle `assembly` / `disassembly` / other file format
handling, and got external passes like the `assembly` integrated in the
compiler's pipeline, by directly emitting machine code, instead of textual
assembly, that is once again parsed by the assembler.

#### Misc

There are many other useful libraries here, like `Orc (JIT)` which provides an
interface to deal with **Just-In-Time** compilation, `LTO` which allows
linkers to perform **link-time optimizations**, `PGO`, etc.

## clang

[clang](https://clang.llvm.org/) is a `C` / `C++` / `Objective-C` front-end
emitting `LLVM IR`.

Like all the sub-projects of the LLVM compiler infrastructure, clang can be
used with modular libraries, and allows a lot of interaction with its
internals allowing easy tool development.

#### libclang

`libclang` is the C API that allows interaction with the source code. It is
widely used, since the C API is more stable than the C++ one.

#### AST

It basically exposes a library to use its `AST`, but also an amazing
[ASTMatchers](https://clang.llvm.org/docs/LibASTMatchers.html) library, which
provides an easy way to match ASTs for tools like refactoring / style-checking,
etc. through a simple and readable syntax:

``` c++
cStyleCastExpr(unless(isExpansionInSystemHeader()));
```

Matches all the explicit casts that don't come from an expansion in the system
headers, like:

``` c++
int j = (int)NULL;
```

#### clang static analyzer

Inside `clang`, lays also the [clang static
analyzer](https://clang-analyzer.llvm.org/) which checks for bugs in source code
by using static analysis through various checks based on `clang`.

#### clang-tidy

Another part of `clang` is [clang-tidy](http://clang.llvm.org/extra/clang-tidy/)
, which detects and can fix various programming errors by following coding-style
guides. It's the perfect tool to use if you need a `C++ linter`.

## lld

[lld](http://lld.llvm.org/) is an `ELF` / `COFF` / `MachO` linker that aims at
better performance and lower memory usage, with readability and modularity in
mind.

`lld` provides a target-independent linker with target-specific hooks for
**relocations** which makes it pretty easy to add support for a new target.

## Others

LLVM Core, `clang` and `lld` are the only projects we're going to look into for
our toolchain. Of course, in order to have a fully complete toolchain, we would
need to add the support of our target to
[compiler-rt](http://compiler-rt.llvm.org/) and
[LLDB](http://lldb.llvm.org/) in order to get some runtime support and a
debugger.

# The architecture: j2

## J-Core

[J-Core](http://j-core.org/) is an open source processor using the
[SuperH](http://j-core.org/) ISA, implemented in VHDL.

The ISA is based on the [SH-2 ISA](http://www.shared-ptr.com/sh_insns.html), and
works as a 5-stage RISC CPU. The architecture has **16-bit instructions** and
**32-bit general purpose registers**. There are 133 instructions and 16 general
purpose registers, along with one 64-bit register.

### Particularities

As every single ISA, there are some particularities to consider. Before looking
into the implementation of the backend, let's take a look at the ISA:

#### Delay slots

Because of the processor's pipeline, and the necessity to complete an
instruction on every cycle, if a branch is taken on a code path, the instruction
after the branch is executed.

```
l1:
    bra l2      ; execute the add, then branch to l2
    add #1, r4  ; this is executed before the branch

l2:
    rts         ; execute the nop, then return
    nop         ; do nothing
```

So, the order of the instructions executed here is `add -> bra -> nop -> rts`.

In order to deal with this issue, we need to fill the instruction slots after
every branching instruction. The slot has to be filled with an extra instruction
that doesn't affect the correctness of the program.

The naive version would be to add a `nop` after every single branching
instruction:

```
l1:
    bra l2     ; branch
    nop        ; delay slot
    add 1, r4  ; never executed

l2:
    rts        ; branch
    nop        ; delay slot
```

But of course, this is not efficient, and we can do better by analysing which
instruction we can push in the extra-slot, in order to avoid adding `nop`s.

#### Addressing modes

Take the following C code:

``` c
int tab[30];
[...]
int a = tab[2];
```

We have here a load of an `int`, at the 3rd position in the array, which is
translated into:

``` c
tab[2] == *(tab + 2 * sizeof (int))
```

The j2 ISA provides the following instructions that can be used to load from
memory:

```
mov.l @Rm, Rn          ; (Rm) -> Rn
mov.l @(R0, Rm), Rn    ; (R0 + Rm) -> Rn
mov.l @(disp, Rm), Rn  ; (Rm + disp * 4) -> Rn
```

In order to use the first one:

```
               ; tab is in r4
mov r4, r1     ; don't modify r4 directly
add 8, r1      ; apply the offset
mov.l @r1, r2  ; load
```

Second one:

```
                     ; tab is in r4
mov 8, r0            ; load the offset
mov.l @(r4, r0), r2  ; load
```

Third one:

```
                     ; tab is in r4
mov.l @(2, r4), r2   ; load
```

Since this is available for 4/2/1-byte loads and stores, we need to select the
address manually, and pick the most appropriate one.

#### Shift

The basic instructions of a logical shift are:

* Rn << Imm -> Rn
* Rn >> Imm -> Rn

but our ISA doesn't support this instruction.

Instead, we have:

```
shll2  Rn  ; Rn << 2  -> Rn
shll8  Rn  ; Rn << 8  -> Rn
shll16 Rn  ; Rn << 16 -> Rn
```

So, we will need to split one shift into multiple constant shifts (if the shift
value is known).

#### Function calls

We would like to call a function with a simple instruction, and the ISA supports
this:

```
bsr label  ; PC + 4 -> PR
           ; disp*2 + PC + 4 -> PC
```

But, there is a small problem here. Label is going to be resolved to the real
label's address in the object file, so it has its limitations:

* J2 has 16-bit instructions, so it's impossible to cover all the address space
  with a displacement encoded in the instruction. For that, `bsr` uses an offset
  relative to the current instruction, encoded on 12 bits, so it can reach from
  -4096 to +4094.

* We don't always know where the callee is, and we can't check if it's in the
  possible range of `bsr`, like in the case of a function in a different
  translation unit.

But, we can use an indirect call to solve this problem:

```
jsr @Rm  ; PC + 4 -> PR, Rm -> PC
```

But in order to use this, we have to fill `Rm` with the absolute address of the
callee, which happens usually at link-time.

#### Misc instructions

Instructions like:

* `xtrct`
* `swap.b`
* `swap.w`
* `rotcl`
* ...

will need to be mapped to intrinsics, since they can't really be matched in any
instruction sequence.

# Implementation

Let's see how to add our target in all the stages required to generate a fully
linked binary.

## clang

### TargetInfo

`clang` is our front-end. Theoretically, the front-end should be separated from
the back-end target, but with C and C++, it's not the case.

`clang` basically emits `LLVM IR` from its `AST`. But the `AST` is so much more
complex and contains many other syntactic sugar expressions, so it will
**lower** the `AST` to `LLVM IR`. During this process, some proprieties of the
C / C++ language need to query the target, so we have to implement our target in
`clang`.

In the target info, we have to decide some target-specific properties that the
standard does not specify, like the size of a pointer, the endianness, the size
of `ptrdiff_t`, etc.

### Misc

* Often, `clang` provides macros to check for the target, the language, the
version, etc. We are going to define the `__J2__` macro if this target is used.

* Inline asm is handled in `clang` as well, and we will need to provide some
  hooks for the constraints, registers, etc.

## LLVM

Here is the core of the project. Here is where we are going to implement the
description of our target, and provide lowering all the way from `LLVM IR` to
assembly or machine code.

I'll skip the boilerplate code, and just focus on the parts of the code
involving LLVM-specific notions and j2-specific ones.

### Registers

Let's start with the description of the registers of our target.

First, we have to describe all the registers that our target has, as long as
their textual representation, their encoding, their register class and their
debug information number.

``` c++
def R0 : J2Reg<0, "r0">, DwarfRegNum<[ 0 ]>;
[...]
def R15 : J2Reg<15, "r15", [ "sp" ]>, DwarfRegNum<[ 15 ]>;

// General purpose registers.
def GPR : RegisterClass<"J2", [ i32 ], 32, (sequence "R%u", 0, 15)>;
```

This all happens in the `J2RegisterInfo` class. This class is queried during
codegen, and this is also where the callee saved registers and the reserved
registers are marked.

### Instructions

Now it's time to describe the instruction set of our target.

The [SH Instruction Set Summary](http://www.shared-ptr.com/sh_insns.html) is a
great resource for this task, since it provides everything we need to implement
in every instruction.

We'll use the following instruction as an example:

##### Load an 8-bit immediate in a register.

```
mov #imm,Rn
```

##### Encoding

```
1110nnnniiiiiiii
```

##### Abstract

```
imm -> sign extension -> Rn
```

#### Instruction format

First, we have the part of the instruction that indicates the operation to be
performed, called an opcode. It is usually followed by operands.

So here we can identify the opcode of the instruction to `0b1110`, or `0xE`,
followed by the operands, which is the destination register encoding on 4 bits,
and the 8-bit immediate.

In order to describe the instruction format, we use `TableGen` classes:

``` c++
def NI : Format<11>;

class FNI<bits<4> op, dag outs, dag ins, string asmstr, list<dag> pattern,
          InstrItinClass itin> : J2Inst<outs, ins, asmstr, pattern, itin, NI> {

  bits<4> dst;            // attribute: the destination register
  bits<8> imm;            // attribute: the 8-bit immediate

  let Inst{15 - 12} = op; // the opcode is placed in the last 4 bits
  let Inst{11 - 8} = dst; // the destination comes right after
  let Inst{7 - 0} = imm;  // and the layout ends with the immediate
}
```

#### Instruction operands

The operands need to be explicitly separated between **input** and **output**
operands of an instruction. It allows LLVM to continue having an abstract
representation after instruction selection and perform transformations and
analysis on the instructions.

For our instruction we would need to set `Rn` as an **output** operand, and
`imm` as an input operand:

```
(outs GPR:$rn), (ins i32imm:$imm)
```

### Instruction selection

Instruction selection in LLVM has its own intermediate representation, called
`SelectionDAG`. First, a DAG is created from the IR, then multiple passes are
done on this DAG to legalize, simplify, and match a pattern that can be
associated to a target instruction.

The instruction selector is then run on every basic block of a function, and it
can select only one basic block at a time.

A new design of for an instruction selector is in progress,
  [GlobalISel](http://llvm.org/docs/GlobalISel.html), which can select
  patterns across basic blocks.

#### Legalization

Legalization is the part of LLVM's code generator that allows targets to decide
whether they support a certain `LLVM IR` instruction, for a certain type, and if
they don't, it allows targets to choose a custom behaviour:

* **Legal**: Supported by the target.

* **Libcall**: Call a function that is part of the runtime, usually provided in
  `compiler-rt`.

* **Promote**: Since a legal operation is tied to its type, promoting the type may
  be giving the instruction another chance to match a legal instruction.

* **Expand**: LLVM has pre-defined expansion of some instructions, based on other
  instructions. This allows all the targets to support mostly all the
  instructions, even if the instruction itself is not legal.

#### Combine

During this pass, the `SelectionDAG` is being optimised by cleaning up trivial
optimisations and usually making it easier for the legalizer and instruction
selector to work.

#### Select

The selection of an instruction is based on a pattern. That pattern is described
as a `SelectionDAG` in the instruction description.

For our instruction, the pattern we would like to match is an assignment to a
32-bit register from a 8-bit immediate, that gets sign-extended:

```
[ (set i32:$rn, i32immSExt8:$imm) ]
```

Here is the final description of the `MOV8ri` instruction:

``` c++
def MOV8ri : FNI<0xE,                                 // opcode
                 (outs GPR:$rn), (ins i32imm:$imm),   // operands
                 "mov\t$imm, $rn",                    // string
                 [ (set i32:$rn, i32immSExt8:$imm) ], // pattern
                 NoItinerary>;
```

This is the instruction as described in `J2InstrInfo`, the class containing
information about the instructions.

### Frame management

The stack is managed by using stack frames.

Here, we have to teach our backend how to emit the **prologue** and the
**epilogue** of a function, as well as the interaction with the callee saved
registers and the stack frame (spill, restore).

``` llvm
define void @foo() nounwind {
entry:
  %tmp = alloca i32            ; allocate stack slot
  store i32 12, i32* %tmp
  %tmp1 = load i32, i32* %tmp
  ret void
}
```

For a simple function, with stack usage, the following prologue will be
generated by the `J2FrameLowering` class:

```
mov.l r14, @-r15  ; prologue: spill r14 (frame pointer)
mov r15, r14      ; prologue: setup the new frame pointer
add #-4, r15      ; prologue: we need 4 bytes for %tmp
mov r14, r0
mov #12, r1
mov.l r1, @r0     ; store
mov r14, r15      ; load
mov.l @r15+, r14  ; epilogue: restore r14 (frame pointer)
rts               ; return
nop               ; delay slot
```

### Calling conventions

During the lowering pass, we need to decide what `ABI` to use and how to handle
function calls, returns, argument passing, etc.

The approach LLVM takes, is pretty nice and modular.

First, you declare one or more calling conventions in `J2CallingConv.td`, by
handling types and associating different actions:

##### J2 calling convention

* Arguments: Registers: `r4`, `r5`, `r6`, `r7`, and then passed on the stack.
* Return values: Registers: `r0`, `r1`, `r2`, `r3`

It is described in LLVM like this:

``` c++
def CC_J2 : CallingConv<[
  CCIfType<[ i1, i8, i16 ], CCPromoteToType<i32>>,
  CCIfType<[ i32 ], CCAssignToReg<[ R4, R5, R6, R7 ]>>,
  CCIfType<[ i32 ], CCAssignToStack<4, 4>> // 4 bytes size, 4 bytes alignment.
]>;
```

So, let's take a look at how this works:

``` c++
CCIfType<[ i1, i8, i16 ], CCPromoteToType<i32>>,
```

First, for types different than `i32`, they have to be promoted to `i32`.

``` c++
CCIfType<[ i32 ], CCAssignToReg<[ R4, R5, R6, R7 ]>>,
```

Then, for `i32` types, try to assign it to the registers decided by the calling
convention.

``` c++
CCIfType<[ i32 ], CCAssignToStack<4, 4>> // 4 bytes size, 4 bytes alignment.
```

But, if all the registers are filled, fallback on the stack.

Once we have all this, the generated code has to be queried during lowering, and
depending on the action decided by the calling convention, we have to create
`DAG` nodes:

``` c++
if (VA.isRegLoc())
  // Copy the argument to the argument register. Update chain and glue.
  Chain = DAG.getCopyToReg(Chain, DL, VA.getLocReg(), Arg, Glue);
```

### Machine code

Since the `AsmPrinter` is not interesting here, because it is generated using
the `J2InstrInfo` where we described the instructions and their textual format,
we are going to talk about directly generating machine code.

#### MCInst

Since we are in the MC layer now, all our `MachineInstr`s have been translated
to `MCInst`s. This is done in `J2MCInstLower`, where we decide what to do for
each instruction, how to describe operands of different types:

* **Registers**: emit the encoding of the register, set in `J2RegisterInfo`.

* **Immediates**: emit the immediate in its binary format.

* **External symbols and local labels**: generate symbol references, resolved
  later in the pipeline.

#### MCCodeEmitter

This is the part where we emit a binary sequence of instructions (opcode,
operands), relocations, symbols, in a file format-independent way.

We have to decide how to emit an instruction, depending on the endianness or
other parameters, how to encode each `MCOperand`, generate relocations for
symbol references, and many other target-specific object file generation.

Here, we make use of the `J2InstrFormat` described as `instruction classes`, to
emit the binary representation of the instruction.

#### AsmBackend

In the `AsmBackend`, we take care of all the `fixups` generated by symbol
references around the code. The `fixups` can be resolved in different ways:

* If the symbol exists in the same object file, apply the fixup by modifying the
  instructions directly.

* If the symbol does not exist, emit a format-specific (here, **ELF**)
  relocation. This relocation will be resolved by the static linker (here,
  `lld`), after the object file generation.

#### Assembler - AsmParser

The MC layer provides an assembly parser tool, that can make use of
`J2InstrInfo` to generate a parser, that may (or not) need some manual work to
adjust some corner-cases of your assembler.

#### Disassembler

Since we provided almost all the necessary information to assemble and
disassemble j2 object code, a disassembler is also easy to make. Based on the
opcodes provided for each instruction, and the `J2InstrFormat`, a basic
disassembler is generated. If your ISA is not trivial, some manual work might be
needed, but for j2, nothing except taking care of the endianness was needed.

## lld

In `lld` we need to care of two things: the driver, and the relocations
generated by our format-dependent object emitter.

### Target Info

The target info contains target-specific information that is queried during
`lld`'s generic algorithms. Basically, here we just handle the ELF machine type.

### Relocations

The relocations are the core part of our `lld` implementation. These relocations
are _holes_ in our code, that could not be filled before this step.

Our hook is the following function:

``` c++
 void J2TargetInfo::relocateOne(uint8_t *Loc, uint32_t Type, uint64_t V) const;
```

Here, we have to store the value `V`, at the address `Loc` depending on the type
of the relocation.

Let's take an example:

```
bsr foo
```

Here, we make a call to `foo`, which could not be found during the assembly
step, since it is an external function. This means that somewhere, in some other
file, we have:

```
.global foo
foo:
    [...]
```

Now that we have the code for `foo`, and the code that calls `foo`, we can
replace the hole in `bsr`'s operand with `foo`'s address.

But it's not that simple, since `bsr` has a special encoding for the operand:

* The operand is an offset starting from the current instruction

* The offset is multiplied by 2 before it is applied to `PC`:

```
disp * 2 + PC + 4 -> PC
```

So for this, we have to tell `lld` that our relocation, of type `R_J2_PC2_12` is
a `PC`-relative relocation, and in `relocateOne`, the value has to be divided by
2 and only the lower 12 bits have to be applied to the instruction.

``` c++
switch (Type) {
  case R_J2_PC2_12: {
    V >>= 1;                               // Divide by 2.
    checkInt<12>(Loc, V, Type);            // Check that it fits in 12 bits.
    Value &= ~((uint64_t)(~0) << 12);      // Clear the 4 high bits.
    write64le(Loc, read64le(Loc) | Value); // Write in little endian.
    break;
  }
```

As you can see, here our offset is limited to +/- 4096, which is not sufficient
for our target.

As we said in the specifics of the target, we need to handle these calls with
indirect calls, using the instruction `jsr @Rm`, which takes a 32-bit address in
a register, as an operand.

#### Dummy solution

We could generate loads for the address as 4 immediates:

```
mov      #0, r0                ; 0b0
shll8    r0                    ; 0b0
or       #0, r0                ; 0b0
shll8    r0                    ; 0b0
or       #1, r0                ; 0b1
shll8    r0                    ; 0b100000000
or       #52, r0               ; 0b100110100
jsr      @r0
nop
```

But, as you can see, the address is split in 4 parts, so each part is going to
need a relocation, and way too many instructions just for a simple function
call.

#### PC-relative mov

But, there is a solution. It seems like our ISA has a `PC`-relative load, that
takes a destination register and an offset from the current `PC`, and loads that
32-bit memory slot in the register.

This is perfect for our usage, since we could insert data at the end of our
functions, that can be referenced from every call in the function!

Let's see how this works: If we want to call `foo`, which is located at the
address `0x134`:

```
10c:    05 d0    mov.l @(5, PC), r0  ; load the address in r0
10e:    0b 41    jsr    @r0          ; call
110:    09 00    nop
[...]
11c:    0b 00    rts
11e:    09 00    nop
120:    34 01    .data.l 0x134       ; raw, never executed, used for the call
```

It seems that the `Mips` and `ARM` backends have similar problems, and handle
this in a similar way.

# TODO

There is a **LOT** more to do. For now, this backend is not even fully
functional, as it can't handle most of the `LLVM IR` instructions.

Here is a quick list of what I plan to implement next:

* Handle most of the `LLVM IR` instructions

* Test.

    * Pass the CodeGen/Generic tests.
    * Try to run this on a real SH-2 CPU + dev board. Hard to find.
    * Try to run this on a FPGA. Easier.
    * Try to run this on QEMU SH-4, and adapt QEMU if needed.

* Handle exceptions.

* Handle shared library `PIC` calls. Implement `PLT` generation in `lld`.

* Provide intrinsics for weird instructions for `clang`.

* Handle debug information.

* Use the 64-bit register `macl:mach`.

* Handle floating-point instructions.

* Implement shrink-wrapping.

* Add a smart implementation of the delay slot filler, that can bring up
  eligible instructions after jumps.

* Add instruction scheduling based on the CPU's pipeline.

* Handle global variables.

* Add a smart constant island finding algorithm.

* Handle inline assembly in `clang`.

* Handle `alloca`.

* Handle `byval` arguments.

* Handle `varargs`.

* Add JIT (`ORC`) support.

* Implement assembler relaxation in MC.

* Add X-Ray support.

* Add a SH-2 ABI compatible subtarget.

* Test the assembler / disassembler.

* Add `LLDB` support.

# More

This project has been presented as a part of [LSE](https://lse.epita.fr/)'s
[lightning talks](https://lse.epita.fr/lt/), and here are the
[slides](https://thegameg.github.io/j2-llvm/).

Similar slides have been presented at the [LSE Summer Week
2016](https://lse.epita.fr/lse-summer-week-2016/), on [the life of an instruction in LLVM](https://thegameg.github.io/llvm-life/).
